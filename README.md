# Quantifying modern inductive biases for deep learning

Inductive biases, broadly speaking, implicitly encourage the machine learning process to prioritize solutions with certain properties. For instance, the classical inductive bias of SGD, studied in this class, favors towards finding minimum norm solutions. Similarly, other definitions of inductive biases have recently been introduced in the deep learning literature. Each having a particular set of assumptions and being specific to a particular stage in the machine learning pipeline, i.e. the training distribution, the choice of architecture, the optimization algorithm, etc.
In this work, we perform 1) a comprehensive review of simplicity biases, identifying their notions of “simple”, main assumptions and investigating possible relationships. Additionally, 2) we experimentally quantify some of these biases across MLPs under different settings.

# Research report

[report_ExperimentalBiases_ivan.pdf](https://github.com/user-attachments/files/16971538/report_ExperimentalBiases_ivan.pdf)
